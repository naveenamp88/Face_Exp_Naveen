{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9eccf7f1-fa6a-4a1f-8d44-527eb86bc0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (22967, 48, 48, 1)\n",
      "Validation data shape: (5742, 48, 48, 1)\n",
      "Training labels shape: (22967, 7)\n",
      "Validation labels shape: (5742, 7)\n",
      "Epoch 1/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.2347 - loss: 1.8367 - val_accuracy: 0.3224 - val_loss: 1.7070\n",
      "Epoch 2/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.3130 - loss: 1.6935 - val_accuracy: 0.4216 - val_loss: 1.5265\n",
      "Epoch 3/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.4078 - loss: 1.5400 - val_accuracy: 0.4530 - val_loss: 1.4156\n",
      "Epoch 4/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.4404 - loss: 1.4485 - val_accuracy: 0.4869 - val_loss: 1.3648\n",
      "Epoch 5/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 48ms/step - accuracy: 0.4698 - loss: 1.3789 - val_accuracy: 0.5091 - val_loss: 1.3239\n",
      "Epoch 6/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 49ms/step - accuracy: 0.4878 - loss: 1.3416 - val_accuracy: 0.5176 - val_loss: 1.2854\n",
      "Epoch 7/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 48ms/step - accuracy: 0.4998 - loss: 1.3144 - val_accuracy: 0.5239 - val_loss: 1.2548\n",
      "Epoch 8/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.5160 - loss: 1.2788 - val_accuracy: 0.5275 - val_loss: 1.2428\n",
      "Epoch 9/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.5224 - loss: 1.2505 - val_accuracy: 0.5348 - val_loss: 1.2246\n",
      "Epoch 10/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.5346 - loss: 1.2302 - val_accuracy: 0.5409 - val_loss: 1.2055\n",
      "Epoch 11/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.5389 - loss: 1.2132 - val_accuracy: 0.5517 - val_loss: 1.1842\n",
      "Epoch 12/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.5490 - loss: 1.1875 - val_accuracy: 0.5446 - val_loss: 1.1981\n",
      "Epoch 13/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 47ms/step - accuracy: 0.5574 - loss: 1.1695 - val_accuracy: 0.5500 - val_loss: 1.1805\n",
      "Epoch 14/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.5547 - loss: 1.1585 - val_accuracy: 0.5540 - val_loss: 1.1905\n",
      "Epoch 15/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.5728 - loss: 1.1379 - val_accuracy: 0.5603 - val_loss: 1.1623\n",
      "Epoch 16/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.5654 - loss: 1.1385 - val_accuracy: 0.5660 - val_loss: 1.1524\n",
      "Epoch 17/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.5788 - loss: 1.1183 - val_accuracy: 0.5662 - val_loss: 1.1479\n",
      "Epoch 18/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.5808 - loss: 1.1151 - val_accuracy: 0.5677 - val_loss: 1.1489\n",
      "Epoch 19/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.5837 - loss: 1.1055 - val_accuracy: 0.5665 - val_loss: 1.1458\n",
      "Epoch 20/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 48ms/step - accuracy: 0.5870 - loss: 1.0857 - val_accuracy: 0.5596 - val_loss: 1.1641\n",
      "Epoch 21/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.5950 - loss: 1.0794 - val_accuracy: 0.5700 - val_loss: 1.1463\n",
      "Epoch 22/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 48ms/step - accuracy: 0.6030 - loss: 1.0574 - val_accuracy: 0.5763 - val_loss: 1.1466\n",
      "Epoch 23/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 46ms/step - accuracy: 0.5996 - loss: 1.0511 - val_accuracy: 0.5749 - val_loss: 1.1331\n",
      "Epoch 24/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.6029 - loss: 1.0346 - val_accuracy: 0.5721 - val_loss: 1.1354\n",
      "Epoch 25/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.6077 - loss: 1.0329 - val_accuracy: 0.5777 - val_loss: 1.1329\n",
      "Epoch 26/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 47ms/step - accuracy: 0.6091 - loss: 1.0281 - val_accuracy: 0.5808 - val_loss: 1.1343\n",
      "Epoch 27/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - accuracy: 0.6139 - loss: 1.0249 - val_accuracy: 0.5763 - val_loss: 1.1345\n",
      "Epoch 28/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.6190 - loss: 1.0072 - val_accuracy: 0.5758 - val_loss: 1.1513\n",
      "Epoch 29/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.6167 - loss: 0.9992 - val_accuracy: 0.5782 - val_loss: 1.1503\n",
      "Epoch 30/30\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.6195 - loss: 1.0010 - val_accuracy: 0.5789 - val_loss: 1.1621\n"
     ]
    }
   ],
   "source": [
    "#Facial Expression Recognition_Palanichamy Naveen\n",
    "\n",
    "# Listing the Contents of the train and test Directories\n",
    "\n",
    "import os\n",
    "\n",
    "# List files in the train directory\n",
    "train_files = os.listdir(r'C:\\Users\\KPRIET\\Downloads\\archive')\n",
    "\n",
    "# Loading Images from the Directory Structure\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = r'C:\\Users\\KPRIET\\Downloads\\archive'\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "# Function to load images from a directory\n",
    "def load_images_from_directory(directory, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        img_path = os.path.join(directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (48, 48))\n",
    "            img = img.astype('float32') / 255.0\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Load the training data\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for label, emotion in enumerate(emotion_labels):\n",
    "    emotion_dir = os.path.join(dataset_path, 'train', emotion)\n",
    "    images, labels = load_images_from_directory(emotion_dir, label)\n",
    "    train_images.extend(images)\n",
    "    train_labels.extend(labels)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Expand dimensions to match the input shape for the model\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Validation data shape:', X_val.shape)\n",
    "print('Training labels shape:', y_train.shape)\n",
    "print('Validation labels shape:', y_val.shape)\n",
    "\n",
    "\n",
    "# Training the Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the model\n",
    "Face_Exp_Naveen = r'C:\\Users\\KPRIET\\Downloads\\Face_ Exp_Naveen.keras'\n",
    "model.save('Face_ Exp_Naveen.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01f678-bb22-440b-ab2f-247be2d9beec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
